\documentclass[14pt,a4paper,oneside]{report}

\usepackage{pdfpages}
\usepackage{epstopdf}
\usepackage{url}
\usepackage{listings}
\usepackage{subfig}
\usepackage{appendix}


\renewcommand{\thesection}{\arabic{section}}
\setcounter{secnumdepth}{3}

\title{\textbf{Optimal decision making for complex problems}
\\ Lunar Lander}

\author{Francois Delarbre \and Simon Lorent}
\makeindex

\begin{document}

\vfill
\date{Academic year 2017 - 2018}

\maketitle
\section{Introduction}
\paragraph{} In this project, we have chose to try to solve the lunar lander\footnote{\url{https://gym.openai.com/envs/LunarLander-v2/}} probleme from openAi gym. To do so, we have tryied two methods, the first one, by adapting code found\footnote{\url{https://www.superdatascience.com/artificial-intelligence/}} for an other problem, which use deep convolutional Q-learning. For the second one we tryied to implement A3C algorithm by ourself. 

\paragraph{} The problem consist of landing the lunar lander on the moon. It always start at the same position but the background change each time the game is played. To land, it may perform four actions, right, left or main engine, or do nothing. It have unlimited fuel so it can fly as long as it may want. 

\section{Deep Convolutional Q-learning}
\paragraph{} Deep convolutional Q-learning, as the name says, make use of a convolutional neural network, which take as an input images of the problem, and output the Q value for each actions. 
\subsection{Eligibility trace}
\paragraph{} The eligibiltiy trace consist of taking in account more of the pasts reward than the normal Q-learning. In traditional Q-learning, one compute the temporal difference as $TD = r_{k+1} + \gamma max\hat{Q}(x_{k+1}, u) − \hat{Q}(x_k, u_k)$. In the case of Eligibility Trace, one use $$TD^{(n)} = r_{k+1} + \gamma r_{k+1} + \gamma^2  r_{k+2} + ... + \gamma^{n-1}  r_{k+n-1} + \gamma^n max\hat{Q}(x_{k+1}, u) − \hat{Q}(x_k, u_k)$$

This allow to take more in account what happened in the past. Thus, the neural network uses the predicted Q values by it output and the computed one to compute the mean squared error loss  to update its weights. 

\subsection{Experience replay}
\paragraph{} For each action taken, the algorithm push to a buffer, this allows to have the benefits of experience replay, by sampling this buffer in order to train the neural network. 
\subsection{Performances}
\paragraph{} After a few epochs, the average reward on the last 100 steps is increasing, and it seems that it has understood that it has to land between the flags, but it seems that it takes longer to understand how to manoeuvring to land there. 
\appendix
\chapter{Installation}
\section{Needed components}
\paragraph{} In order to run the code, it is needed to install first openAI gym lunar lander v2 environement, then tensorflow is needed for the A3C part, the deep convolutional Q-learning requiere pytorch, cuda version to be installed. And thus it need a working version of cuda installed on a linux machine. 
\end{document}
